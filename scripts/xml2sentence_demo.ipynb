{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML 2 Sentence Format Demo\n",
    "\n",
    "This notebook is for showing how to convert the XML format to sentences for training in other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m210842/opt/miniconda3/envs/nlpy37/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* loaded all libraries\n"
     ]
    }
   ],
   "source": [
    "import medtator_kits as mtk\n",
    "import sentence_kits as stk\n",
    "\n",
    "# force reload everything in mtk\n",
    "import importlib\n",
    "importlib.reload(mtk)\n",
    "importlib.reload(stk)\n",
    "\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# for display nicer\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# load sentence detection\n",
    "import pysbd\n",
    "# load spacy and config the sentencizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "print('* loaded all libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* checking path ../sample/ENTITY_RELATION_TASK/ann_xml/Annotator_A/\n",
      "* parsed XML file ../sample/ENTITY_RELATION_TASK/ann_xml/Annotator_A/A_doc2.txt.xml\n",
      "* parsed XML file ../sample/ENTITY_RELATION_TASK/ann_xml/Annotator_A/A_doc3.txt.xml\n",
      "* parsed XML file ../sample/ENTITY_RELATION_TASK/ann_xml/Annotator_A/A_doc1.txt.xml\n",
      "* parsed XML file ../sample/ENTITY_RELATION_TASK/ann_xml/Annotator_A/A_doc4.txt.xml\n",
      "* checked 4 files\n",
      "* found 4 XML files\n",
      "* skipped 0 non-XML files\n",
      "{'total_files': 4, 'total_xml_files': 4, 'total_other_files': 0, 'total_tags': 52}\n"
     ]
    }
   ],
   "source": [
    "path = '../sample/ENTITY_RELATION_TASK/ann_xml/Annotator_A/'\n",
    "rst = mtk.parse_xmls(path)\n",
    "print(rst['stat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and convert format\n",
    "\n",
    "We want to convert the text into a sentence-based format for downstream task (e.g., training relation extraction), so first of all, a sentencizer and a tokenizer are needed.\n",
    "You can use any libraries for this purpose.\n",
    "\n",
    "For here, we use `pySBD` for [sentence boundary detection](https://github.com/nipunsadvilkar/pySBD).\n",
    "\n",
    "```Python\n",
    "import pysbd\n",
    "text = \"My name is Jonas E. Smith. Please turn to p. 55.\"\n",
    "seg = pysbd.Segmenter(language=\"en\", clean=False, char_span=True)\n",
    "print(seg.segment(text))\n",
    "# [TextSpan(sent='My name is Jonas E. Smith. ', start=0, end=27), TextSpan(sent='Please turn to p. 55.', start=27, end=48)]\n",
    "```\n",
    "\n",
    "and we use `spaCy`'s [Tokenizer](https://spacy.io/api/tokenizer).\n",
    "\n",
    "```Python\n",
    "\n",
    "# Use Sentencizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "tokenizer = nlp.tokenizer\n",
    "tokens = tokenizer(\"This is a sentence for tokens.\")\n",
    "sentence_tokens = list(map(lambda v: v.text, tokens))\n",
    "sentence_tokens\n",
    "# ['This', 'is', 'a', 'sentence', 'for', 'tokens', '.']\n",
    "```\n",
    "\n",
    "Second, we need a way to map the spans to token index.\n",
    "In the `sentence_kits.py`, we implemented a function `update_ents_token_index()` for this purpose. It uses the spans of a tag to check whether overlapped with any tokens of a sentence.\n",
    "The function `update_ents_token_index()` will update the entities by adding a new property `token_index` which is a list of token indexes.\n",
    "\n",
    "For more details, please check the follwoing section *Sentence-based format* and the source code in `sentence_kits.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence-based format\n",
    "\n",
    "In the following demo, we will convert each XML file into a sentence-based format, which looks like the following:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"text\": \"The full text of the file\",\n",
    "    \"sentence_tags\": [{\n",
    "        \"sentence\": \"this is a sentence.\",\n",
    "        \"sentence_tokens\": [\"this\", \"is\", \"a\" \"sentence\", \".\"],\n",
    "        \"spans\": [start, end],\n",
    "        \"entities\": [{\n",
    "            \"id\": \"A1\",\n",
    "            \"text\": \"a sentence\",\n",
    "            \"token_index\": [2, 3]\n",
    "            // \"token_index\": [start_token_idx, end_token_idx]\n",
    "        }],\n",
    "        \"relations\": [{\n",
    "            // the same format as relation in XML\n",
    "        }]\n",
    "    }]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* got 4 ann_sents\n"
     ]
    }
   ],
   "source": [
    "# let's check the sentence-based format for the given samples\n",
    "\n",
    "ann_sents = stk.convert_anns_to_sentags(rst['anns'])\n",
    "print(\"* got %s ann_sents\" % (len(ann_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** 0 ******************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "[ `On` | `28Jan2021` | `18:30` | `,` | `the` | `patient` | `experienced` | `<span style=\"color:#EFD9E9;background:black;\">mild</span>` | `<span style=\"color:#CBEDAB;background:black;\">nausea</span>` | `.`]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** 1 ******************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "[ `The` | `day` | `after` | `the` | `arm` | `was` | `<span style=\"color:#9EEFFF;background:black;\">very</span>` | `<span style=\"color:#ABACFC;background:black;\">sore</span>` | `<span style=\"color:#ABACFC;background:black;\">,</span>` | `<span style=\"color:#ABACFC;background:black;\">red</span>` | `<span style=\"color:#ABACFC;background:black;\">and</span>` | `<span style=\"color:#ABACFC;background:black;\">bruised</span>` | `,` | `but` | `he` | `thought` | `it` | `is` | `because` | `the` | `administration` | `of` | `the` | `needle` | `,` | `not` | `a` | `side` | `effect` | `of` | `the` | `vaccine` | `.`]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** 2 ******************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "[ `The` | `consumer` | `reported` | `receiving` | `her` | `first` | `injection` | `of` | `the` | `Pfizer` | `BioNTech` | `COVID-19` | `vaccine` | `2` | `weeks` | `ago` | `and` | `the` | `only` | `side` | `effect` | `she` | `experienced` | `was` | `some` | `<span style=\"color:#BCCF9E;background:black;\">mild</span>` | `<span style=\"color:#99EADF;background:black;\">pain</span>` | `at` | `the` | `site` | `of` | `injection` | `that` | `lasted` | `only` | `a` | `day` | `.`]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[ `The` | `consumer` | `reports` | `the` | `<span style=\"color:#BACBCE;background:black;\">pain</span>` | `was` | `<span style=\"color:#D9AEDA;background:black;\">similar</span>` | `to` | `what` | `she` | `experienced` | `in` | `the` | `past` | `after` | `a` | `flu` | `shot` | `and` | `a` | `shingles` | `shot` | `.`]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[ `The` | `outcome` | `of` | `the` | `event` | `<span style=\"color:#DCCEAD;background:black;\">mild</span>` | `<span style=\"color:#FC9BBB;background:black;\">pain</span>` | `at` | `the` | `site` | `of` | `injection` | `was` | `recovered` | `while` | `other` | `event` | `was` | `unknown` | `.` | ` `]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** 3 ******************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "[ `Her` | `<span style=\"color:#FDDADE;background:black;\">mild</span>` | `<span style=\"color:#DBEBCF;background:black;\">headache</span>` | `continues` | `since` | `the` | `vaccine` | `but` | `is` | `somewhat` | `improved` | `.`]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[ `She` | `denies` | `worsening` | `or` | `<span style=\"color:#9EF9F9;background:black;\">severe</span>` | `<span style=\"color:#CBCAFA;background:black;\">headache</span>` | `;` | `denies` | `<span style=\"color:#BBABB9;background:black;\">vision</span>` | `<span style=\"color:#BBABB9;background:black;\">changes</span>` | `,` | `<span style=\"color:#BBAADE;background:black;\">dizziness</span>` | `,` | `<span style=\"color:#FFCAFD;background:black;\">lightheadedness</span>` | `,` | `<span style=\"color:#ABECD9;background:black;\">speech</span>` | `<span style=\"color:#ABECD9;background:black;\">difficulty</span>` | `,` | `etc` | `.` | `  \n",
       "\n",
       "`]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's show how the results look like\n",
    "# the following code just for reference, you can use and modify for your purpose\n",
    "for ann_idx, ann_sent in enumerate(ann_sents):\n",
    "    print('*' * 30, ann_idx, '*' * 30)\n",
    "    for sentag in ann_sent['sentence_tags']:\n",
    "        if len(sentag['relations'])>0:\n",
    "            tokens = copy.copy(sentag['sentence_tokens'])\n",
    "            # print(tokens)\n",
    "            for ent in sentag['entities']:\n",
    "                color = ''.join([random.choice('9ABCDEF') for j in range(6)])\n",
    "                for idx in range(ent['token_index'][0], ent['token_index'][1] + 1):\n",
    "                    tokens[idx] = '<span style=\"color:#%s;background:black;\">%s</span>' % (\n",
    "                        color,\n",
    "                        tokens[idx]\n",
    "                    )\n",
    "            display(HTML(\"[ `\" + \"` | `\".join(tokens) + \"`]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each token is shown in a pair of ``. \n",
    "The entities are located by the token index."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPy37",
   "language": "python",
   "name": "nlpy37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
